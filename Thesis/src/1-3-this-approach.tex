\subsection{This Approach}
  To improve the task of how training data are generated the whole process of how such data are crafted has to be analyzed. Besides the impact of the \ac{UI} that is used to annotate data, the annotation process of currently available tools itself is what we challenge. We developed a new general purpose annotation framework: It is customizable to fit any kind of data (format) and desired \ac{UI} and provides an annotation work cycle to support \ac{AL} and (constantly improving) pre-annotations from a \ac{ML} unit. These suggestions should assist the annotators and thus increase efficiency. We evaluated this benefit in detail in Section~\ref{sec:method} et seqq.

  \paragraph{Research Question of This Work}
  We aimed to improve the task of training data generation by asking ourselves how this time consuming process can be made more comfortable and faster. We want to assist the annotators doing their task. We want to increase the efficiency of the task of annotating texts. Besides many \ac{UI} improvements we focused on the idea of shifting the task from annotating to supervising. This way the research question we developed was: Can a system compute annotations that help to improve the task of manual data labeling? If we utilize an \ac{AL} approach to support annotators, will it result in more correct, faster and more complete annotations? This leads to the question of what efficiency means in this context and how it is operationalized.

  \paragraph{Efficiency and Ways of Increasing it}
  \label{sec:approachEfficiency}

  To define the word efficiency first and to delimit it from effectiveness:

  \epigraph{It is fundamentally the confusion between effectiveness and efficiency that stands between doing the right things and doing things right.}{\textit{Peter F. Drucker}~\cite{drucker1963effectiveness}}

  Or in other words: Effectiveness is a measurement of reaching a goal altogether, whereas efficiency describes wasting no resources while doing a certain task or creating as much of a desired outcome as possible. \\
  Increasing efficiency therefore means to gain more outcome for the same effort, or for the mission of improving training data generation: Getting \textit{more} text annotated in the same amount of time (reducing the time spent per annotation) or getting more \textit{correct} annotations done in the same time -- and in the best case both at once. As seen in Section~\ref{sec:currentState}, the available tools often lack a clean, simple and straight forward \ac{UI}: They are cluttered with control elements and because they show the whole document at once (rather than only singular extracts to work on at a time) it is hard to focus on what is important (which would be only one sentence and its corresponding labels).

  \pagebreak

  Reducing the number of functions and control elements in the \ac{UI} to a minimum (nothing but the necessary), not overloading the annotators with information but rather presenting one text extract at a time is in line with usability guidelines to improve the usability of such an interface~\cite{galitz2007essential}.\footnote{We aimed to provide a full stack efficiency improving tool chain with our annotation framework, at least for training data generation for the \ac{NER} task. Therefore we developed a \ac{UI} with these guidelines in mind. It remains to the interested reader to evaluate this interface to see if our attempt of improving efficiency with our \ac{UI} was successful.}

  \label{sec:efficiencyAL}
  Another way to improve annotation efficiency can be done by computing a metric of importance ranking each part of the whole data. All the documents that need to be annotated can then be ordered by using this metric in a \textit{most-impact-first} manner. Creating training data sets that contain only data that would \lqq help\rqq the model to improve a lot would make it unnecessary for annotators to process passages that would not improve the model significantly. This intentional skipping of non-effective work would save time and thus improve efficiency. Many approaches to this metric are possible: A model can be trained and a confidence score for each label prediction can be calculated. A very low confidence would indicate a necessary human examination. However, this work focuses on the third element we identified as important for a new annotation environment: the annotation assistance system.

  \paragraph{The Assistance System}
  To answer the research question this work focuses on the assistance system. In particular: on how it helps to improve the efficiency of the annotation task. As already mentioned, the system should make suggestions of annotations that only have to be approved by a human annotator. Our main hypothesis was that overseeing suggestions and correcting them as appropriate will lower the cognitive demands of the annotators and therefore accelerate and improve the annotation task with respect to correctness.

  If we can achieve this, we would be able to create more training data sets, since we generally lower the cost of producing them. This would allow us to create less general but more domain specific training data, which would impact the performance of current \ac{ML} applications: For example the performance of \ac{NER} systems on German texts is worse than the one on English texts, on one part because for English texts there is far more training data available~\cite{agerri2016robust}. The performance could be improved if we could train several specialized \ac{NER} models for different text or topic domains. This is currently done infrequently due to a lack of diverse but large enough training data sets. If our hypothesis about our annotation framework holds, we would be able to create training data more rapidly and therefore more specialized for their applications.

  \singleFullSizeFig{1/AssistanceSystemFlow.png}{The annotation workflow: The assistance system works on the plain text before it will be displayed to a human annotator. Its impact should improve the work of the human.}{structureAnnotationWorkflow}

  The general idea of the assistance, from a procedural point of view, is to predict new annotations (as suggestions) to lighten the workload of the annotators. See Figure~\ref{fig:structureAnnotationWorkflow} for a schematic flow. The human work as well as the assistance system itself is a source of error. We nonetheless assume that the assistance system has a positive influence on the quality of the annotated texts and the time spent by the human annotator.

  To reach this goal we want to split the entire process into several smaller parts: After each iteration the assistance can learn more and make better predictions over time. Such an assistance system will make mistakes. Its prediction will get better over time, but it will never be perfect (if it were perfect, there would be no need for training data to create such a perfect system). We assume that such an assistance, even if it makes mistakes, will nevertheless assist the human (see hypothesis in Section~\ref{sec:hypotheses}).

  The rest of this thesis is structured as follows: We describe the conception, the implementation and the mathematical background of our implementation of an assistance system providing named entity pre-annotations. This implementation serves as an example of the assistance as described. We continue with a detailed explanation of how we conducted the study to evaluate the impact of such an assistance on the human annotation performance. Subsequently we present the results of this evaluation and a detailed discussion of these results.
