\subsection{Conception}
	Understanding the needs of the users of this tool is key to make it helpful. Target users of this software are data scientists (the annotation process itself can be sourced out to be done by non-professionals as well). We expect the user group to know some programming as we require them to convert their data so that it matches our interfaces. This allows us to construct the framework in a more flexible manner as we can expect our users to adjust their own software to the framework's interfaces.

	\subsubsection{Requirements of the Framework as a Whole}

		\singleFullSizeFig{2/simple-dalphi-work-cycle.png}{Visualization of the \acl{DALPHI} work cycle.}{SimpleDalphiWorkCycle}

		Most of all, the framework should provide a unified workflow cycle (illustrated in Figure~\ref{fig:SimpleDalphiWorkCycle}): Uploading and organizing data, annotating data, processing data and again starting a new iteration of annotating data are the core steps. It is not about annotating everything at once, it is about dividing the task into several iterations. Annotators will not be overwhelmed by the amount of data to annotate. This allows the system to be retrained on the new data and to then make predictions between each iteration. One iteration refers to the time an annotator spends on annotating data. This period can cover the annotation of many, but at least one complete document.

		The suggestions of the assistance are not created live in the very moment in which an annotator is working with the data, but after s/he is done with the current iteration. The annotator can decide to stop an ongoing iteration after each processed document. When an iteration finishes, the systems will take all the annotated data from the just finished iteration (and all the previous iterations), create a model from these annotations and make predictions about the remaining, not already annotated data. These documents will be reviewed by the human annotator in the next iteration.

		In the following iteration, again a fraction of the whole data will be annotated. But this time, the human annotator is correcting wrong predictions and completing misses of the system. After this second iteration, the assistance component will again learn from the changes made and improve itself; this time it can create an even better model from the data because it has more and refined information in the training data. This way it will make more accurate predictions and will find annotations it has missed in the previous iteration -- annotations that haven't been reviewed by human annotators yet.

		To provide this learning mechanism not only for \ac{NER} related data, the crucial elements (the \ac{UI}, the assistance component) have to be problem agnostic. The whole framework should be suitable to create training data for any kind of problem. This implies that the mentioned components are not shipped with the framework; they have to be written by the users of the framework.\footnote{We provide an OpenSource collection of examples for different use cases. They can be modified and adapted -- and provide an easy access to the framework.} Thus the \ac{API}s need to be relatively easy to adapt in order to lower the adaption costs for users. Other important design decisions are:

		\begin{enumerate}
			\item The framework has to be modular to easily change crucial components, like the \ac{UI} or the assistance system. This is needed for rapid testing during the development of these components and in order to be flexible for different tasks.
			\item The assistance should be invisible to the user (a background task). It should learn without any preparation or intervention by the user.
			\item The framework's \ac{UI} and \ac{API}s should be as simple as possible.
			\item To satisfy the demand of flexibility (create training data not only for \ac{NER} but for all supervised \ac{ML} tasks), this \ac{UI} should be easily exchange- or replaceable.
			\item The framework should be open source from the very beginning. This hopefully helps growing the user base and getting contributions from other data scientists to grow the number of available \ac{ML} and \ac{UI} components and increase the feature set of the existing ones. We hope to receive collections of training data sets as well, all using the same format to provide as many tools and resources for new users as possible.
		\end{enumerate}

		We created the \ac{DALPHI} framework having the requirements above in mind. The framework itself and its components are free software and can be found on GitHub.\footnote{\ac{DALPHI} on GitHub: \urlWithDate{https://github.com/Dalphi} Further details on the development and can be found in the Appendix~\ref{appendixDalphiDev}.}

	\subsubsection{\acl{AL} at Heart}
		\label{sec:conceptionNERatHeart}
		% NER is the technological foundation; it learns entities and makes predictions; there are different NER classifiers
		The key requirement for the assistance is to learn and to improve iteratively. It uses the provided \ac{ML} algorithm frequently to train a better model with each step.\footnote{The time to train a model depends heavily on the task and the algorithm. Some need only seconds, other several hours or even days for one training phase. In such cases project owners are likely to start a new iteration only if a useful amount of new annotations were gathered. Therefore \ac{DALPHI} provides different user roles: An annotator account is not able to start a new iteration, only an admin account is. This way only project responsible members decide when to start new iterations.}
		That said, the assistance is nothing but the multiple (iterative) application of the same \ac{ML} technique. In the case of training data generation for \ac{NER} on which we focus within this work, the underlying algorithm for the assistance system is one of the standard \ac{NER} classifiers (see Section~\ref{sec:algorithmicFoundationMEC} for an explanation of the classifier we used in detail).\footnote{This classifier is exchangeable and could easily be replaced by another \ac{NER} classifier or a completely different algorithm for a different use case.}
		A \ac{NER} classifier labels every word of a text as a named entity (if any)~\cite{nadeau2007survey}. Usual labels are names of people, organizations / company names or locations~\cite{tjong2003introduction}.

		% the assistance is AL
		The way the \ac{ML} algorithms are used is similar to a classical \acf{AL} approach: The machine predicts new labels and a human supervisor controls the process by correcting, adding or accepting the predictions of the model~\cite{olsson2009literature, settles2010active}. Furthermore the human will only be asked if the algorithm's confidence for one or more of the pre-annotations that are presented at once are below a certain threshold. This way the human \lqq helps out\rqq in difficult situations whereas the algorithm will label the data in easy cases on its own. Since this workflow can be applied to many different kinds of \ac{ML} training data generation, this assistance / \ac{AL} combination is where the acronym \acl{DALPHI} comes from.

	\subsubsection{Data Flow Inside of \acl{DALPHI}}
		\label{dalphiMergeService}
		\singleFullSizeFig{2/dalphi-data-flow-cycle.png}{The processing and data flow cycle of the \ac{DALPHI} framework. Green documents are annotated, black documents are not.}{DalphiWorkflow}

		Figure~\ref{fig:DalphiWorkflow} describes the four parts the \ac{DALPHI} framework consists of: The framework itself with its \acs{API}s, an Iteration Service (1.), an annotation \ac{UI} (2.)~and a merge component (3.) -- all of which we will discuss in detail. The numbered components (1. - 3.) have to be provided by the framework's users. This is because only they themselves will know the structure of their data and how they have to be processed. Most of all, the \ac{DALPHI} framework provides a workflow and unifying \ac{API}-interfaces.

		The first task of using the framework will be uploading all the data that should be annotated via the web interface of the framework itself. These uploaded data are called \textit{raw data} (pure, raw data without any annotations). The following steps describe the first iteration (in the \ac{NER} scenario):

		\paragraph{1. The Iterate Service with the Assistance Component}
		The iteration service will convert and split the whole corpus (the set of all the texts that should be annotated) it received into many \textit{annotation documents}. This is because it can be assumed that the amount of all of the \textit{raw data} is a lot more than one annotator could handle at once. So the whole corpus is being divided into many small passages a human annotator is easily capable of annotating. What the exact rule of division is depends on the data. As a rule of thumb, whole paragraphs will work: On the one hand they contain most information needed to understand what a passage is about (they provide some context), on the other hand they are usually not too large.

		Together with this splitting process, the assistance component will be applied to the yet unlabeled \textit{annotation documents} for the first time -- if it has a built in bootstrapping fallback: The idea here is to provide a pre-trained model that can predict labels even if the data set is fully unlabeled. Such a general purpose model (not specialized on a certain text domain) will not perform as good as a specialized model but it would catalyze the process in the beginning. After the data set has been labeled to an extent that a model based on the data set would perform better than the general purpose model, the iteration component would stop using the bootstrap method and continue with the self trained model instead. If the implementation of the assistance does not have such a bootstrapping component, it will simply return each \textit{annotation document} without any changes.

		\paragraph{2. The \acl{UI}}
		A \ac{UI}, compatible with the created \textit{annotation documents}, has to be provided to render the documents. This interface consists of two to three files: An \ac{HTML} file is the basis of the interface and defines a hierarchical structure of everything that is visible within the interface. A \ac{JS} file will provide client side software (which runs in the browser of the annotators) to handle user interactions like mouse clicks. It furthermore contains an encoding function, which we'll discuss in detail below. Optionally (but most certainly) a \ac{CSS} file can be provided. This file contains styling instructions defining how elements of the \ac{HTML} file should look like (and to a certain extend how they should behave as well, e.g.~hover effects or animations). Using these files, the interface renders the text of each document and integrates control elements to manipulate or generate annotations. It acts like a template, filling itself with data from the framework.

		Therefore, the framework's client side first loads all the interface files and then requests the next \textit{annotation document} from the framework's \ac{DB}.\footnote{The \textit{next annotation document} is always one that has not been processed yet and might follow an order like the importance of a document. The \ac{DALPHI} framework provides a corresponding \acs{API} endpoint.}
		It then renders the \ac{UI} with the data of the current \textit{annotation document}. After the annotation of one document is complete, the \ac{UI}'s annotation encoder is called to transform all the changes that have visually been made with the \ac{UI} back into the \textit{annotation document} format. The particular encoding is part of the \ac{UI} component because the author of the interface code knows best how to transform the data back into the exchange format. The only requirement the \ac{DALPHI} framework makes about this data format is to use \ac{JSON}. \ac{JSON} is a commonly used exchange format in web applications, mostly for structured text data (but it's even possible to use it for binary files like images if such files get pre-encoded using Base64 for example).\footnote{Although it should generally be avoided to use binary files in \ac{JSON} code because of the overhead of the special encoding and its consequential need of additional computing resources.}

		The annotation \ac{UI} processes only one \textit{annotation document} at a time. After one document is annotated and saved, the encoded text annotation combination will be handed back to the framework's \ac{DB}; the next document will be loaded and annotated. This process is repeated until the user decides that the first annotation iteration is complete.

		\paragraph{3. The Merge Service}
		If the iteration is finished -- the annotator decided that the annotation process is completed for now -- all the annotated \textit{annotation documents}, together with their belonging \textit{raw data} documents, are sent to a \textit{Merge Service}. This is to merge the annotations (that are stored in the \textit{annotation documents}) into the corpus (in which users are actually interested in). Up until this point the corpus is not annotated. After the merging task is completed, all the current \textit{annotation documents} are deleted and the corpus is updated.

		To summarize this process: One iteration entails splitting the corpus in processable, small blocks, annotating them and finally merging them again with the original data set.

		When the next iteration starts, the now partly annotated corpus is sent to the assistance system in order to be split and pre-annotated again. This component will now train a model with the annotations that the corpus now contains, make predictions about not annotated entities in the text and generates a new pool of \textit{annotation documents}. This process is repeated until the whole text is annotated or a certain model performance can be achieved with the training data.\footnote{To measure how well the model currently is, a further component can be integrated into the framework. This would take the corpus as it is and train one or more models (with different algorithms) which will be cross-validated to return a performance index like an F-score or a precision / recall ratio.}
